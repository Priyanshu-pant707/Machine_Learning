# cost function :

-  calculate the residual error 
-  sum all the residual error.
-  sqaure the difference , so that the values in positive.
-  find the average of the residual error, 

$$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
$$


# Gradient descent :

- we have to take global minima
- least error
- use for making best fit line.


# Repeat conversion theorem & learning Rate

- learning rate should be minimum,
- if the learning is high the value of the loss fuction varies at high rate.
 